---
theme: white
transition: convex
---

<!-- .element: class="fragment" -->
# Paralelismo no Hardware
## Aula 11

---

## üèóÔ∏è 1. Multi-Core (M√∫ltiplos N√∫cleos)

Diferente do passado, onde havia um √∫nico n√∫cleo saltando entre aplicativos (Context Switch), hoje temos v√°rios n√∫cleos f√≠sicos no mesmo inv√≥lucro (Chip).

- **Core F√≠sico:** √â uma CPU completa e independente, com sua pr√≥pria ALU, Unidade de Controle e Caches L1/L2 particulares.
- **Cache L3 Compartilhado:** Na maioria dos designs AMD e Intel reais, os M√∫ltiplos Cores (Ex: 8 Cores) conversam e trocam estados atrav√©s de uma suntuosa e lenta √°rea comum L3 que circunda todos os processadores ali impressos no wafer.

> [!TIP]
> **Em Backend pesado:** Se o banco mapear duas Threads puras `backend` em dois Cores puramente isolados (Ex: Core 0 e Core 1), e elas lerem/trabalharem na mesma matriz cont√≠nua, o Hardware for√ßar√° interc√¢mbios el√©tricos no *Cache Coherence Protocol (MESI)* rodando por toda placa m√£e. Fiquem espertos com o *False Sharing*! 

---

---

## üß¨ 2. Hyper-Threading (SMT - Symmetrical Multi-Threading)

A m√°gica comercial da Intel e AMD nos anos 2000. Como fazer "1 Core F√≠sico" fingir ser "2 Cores L√≥gicos" para o Windows/Linux?

Na aula 03, vimos que a execu√ß√£o cruza pelo Pipeline ou pode esbarrar em ciclos ociosos na CU aguardando a Mem√≥ria Principal. O *Hyper-Threading* espeta um **Segundo conjunto de Registradores** e Hardware de Estado no mesmo Core. Enquanto o c√≥digo da Thread "A" est√° 0.5 nanosegundo *travada* esperando chegar o dado lento da L3, o Core troca instantaneamente para o contexto da Thread "B", executando-o usando as mesmas Unidades L√≥gicas (ALU) num aproveitamento fabril monstruoso de 100%.

<div class="termy" markdown="1">

```console
$ # Lendo o processador em Linux (Ex: i7 4-Core com HyperThreading)
$ lscpu
CPU(s):                  8
On-line CPU(s) list:     0-7
Thread(s) per core:      2
Core(s) per socket:      4
```

---

## üéÆ 3. GPUs: O Paralelismo Maci√ßo

CPUs (Processadores) foram feitos para "Serem R√°pidos executando sequ√™ncias l√≥gicas e IFs complexos". Possuem Caches gigantes.
GPUs (Placas de V√≠deo) foram feitas para "Executar a MESM√çSSIMA MIN√öSCULA matem√°tica simultaneamente em milhares de pixels fracos". Sem grandes condicionais, focando no *Throughput*. 

NVIDIA e CUDA (plataforma de C++) reinam supremas em *Deep Learning* e Criptografia exatamente porque pegam *Loops For* gigantescos de √Ålgebra Linear, e fracionam em **8.000 mini-n√∫cleos (CUDA cores)** esmagando qualquer Intel Core i9 na lat√™ncia matem√°tica cont√≠nua pura.

---

## üöÄ Resumo Pr√°tico

- **Task Paralelism**: Se tens l√≥gica variada, use a *CPU Multi-Core C++ thread pool*.
- **Data Paralelism**: Se a conta for a repeti√ß√£o retumbante de um algoritmo id√™ntico sobre 2 milh√µes de dados sem depend√™ncia de saltos complexos, mova-a da RAM √† VRAM da *GPU via CUDA/OpenCL*. A m√©trica vai das horas paras os d√©cimos de segundo.

