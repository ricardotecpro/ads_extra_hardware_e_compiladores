# Aula 05 - Hierarquia de Mem√≥ria

A mem√≥ria √© o maior gargalo oculto no software moderno de alta concorr√™ncia. Quanto mais pr√≥ximo o dado est√° da CPU, mais r√°pido √© o acesso. Mas a velocidade custa dinheiro e escala t√©rmica.

---

## üèõÔ∏è 1. A Pir√¢mide de Alta Performance

Um programador ing√™nuo acha que "vari√°vel vai na mem√≥ria". Um engenheiro de software C/C++ sabe *em qual camada* a vari√°vel se hospeda:

```mermaid
graph TD
    A("Registradores<br/>(1 Ciclo - Alguns Bytes)") --> B["Cache L1<br/>(~4 Ciclos - ~64KB a 128KB)"]
    B --> C["Cache L2 e L3<br/>(~12 a ~40 Ciclos - Megabytes)"]
    C --> D[["RAM (Mem√≥ria Principal)<br/>(~200 a ~300 Ciclos - Gigabytes)"]]
    D --> E[("Armazenamento (SSD / HDD)<br/>(Milh√µes de Ciclos - Terabytes)")]
    
    style A fill:#ff9999
    style B fill:#ffcc99
    style C fill:#ffff99
    style D fill:#ccffcc
    style E fill:#99ccff
```

> [!IMPORTANT]
> A lat√™ncia √© o tempo que demora da CPU pedir um dado at√© ele chegar. Buscar um byte da **RAM** demora ~200 ciclos. Buscar do **SSD** demora centenas de milhares. Essa diferen√ßa grotesca √© mitigada pelo uso de Caches.

---

## ‚è≥ 2. Os Impactos da Lat√™ncia (Lado do C√≥digo)

Quando escrevemos um c√≥digo com constantes consultas n√£o linearizadas ao Banco de Dados (ou SSD local), pagamos a mais cara taxa processual: o I/O disk penalty.

<!-- termynal -->
```console
$ # Como consultar as camadas do processador Linux
$ lscpu | grep Cache
L1d cache:                       64 KiB
L1i cache:                       64 KiB
L2 cache:                        1 MiB
L3 cache:                        12 MiB
```

A instru√ß√£o e os dados descem da L3, saltam para L2, descem para L1 e se acoplam na ALU.

---

## üéØ 3. Optimizando Uso

Por que linguagens como C e C++ dominam infraestrutura de servidores High Frequency Trading?
Porque elas permitem `Aloca√ß√£o Est√°tica e Constante` que √© perfeitamente "encaixada" pelo compilador diretamente na mem√≥ria **Cache**.

Ao inv√©s de carregar gigabytes de *Strings* na lenta RAM, as linguagens de baixo n√≠vel incentivam o uso de matrizes de tamanho delimitado (arrays fixos), cujo agrupamento cont√≠guo for√ßa a arquitetura de **Hardware Prefetching** a adiantar os bytes do Array para a Cache nativamente, antes mesmo de voc√™ rodar a linha do c√≥digo!

## üöÄ Resumo Pr√°tico

- Se processadores hoje s√£o m√≠sseis atingindo +4GHz, a RAM parou no tempo (Lat√™ncia de CAS n√£o baixa proporcionalmente).
- Tudo recai na t√©cnica humana de amarrar dados juntos (Caches L1 e L2) e escrever *data-oriented code* se quiser ultra-lat√™ncia C++.



---

## üéØ Pr√≥ximos Passos

<div class="grid cards" markdown>

-   :octicons-video-24: **Acessar Slides**

    ---
    
    Reveja a apresenta√ß√£o visual desta aula.
    
    [:octicons-arrow-right-24: Ver Slides da Aula](../slides/slide-05.html)

-   :octicons-tasklist-24: **Quiz**

    ---
    
    Teste seu entendimento b√°sico com perguntas r√°pidas.
    
    [:octicons-arrow-right-24: Responder Quiz](../quizzes/quiz-05.md)

-   :octicons-pencil-24: **Exerc√≠cios**

    ---
    
    Pr√°tica avan√ßada e dissertativa com consulta.
    
    [:octicons-arrow-right-24: Lista de Exerc√≠cios](../exercicios/exercicio-05.md)

-   :octicons-rocket-24: **Projeto**

    ---
    
    Laborat√≥rio pr√°tico de codifica√ß√£o em C/C++.
    
    [:octicons-arrow-right-24: Mini Projeto](../projetos/projeto-05.md)

</div>


[:octicons-arrow-right-24: Avan√ßar para Aula 06](aula-06.md){ .md-button .md-button--primary }
