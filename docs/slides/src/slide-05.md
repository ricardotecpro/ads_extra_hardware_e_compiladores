---
theme: white
transition: convex
---

<!-- .element: class="fragment" -->
# Aula 05 - Hierarquia de MemÃ³ria
## ApresentaÃ§Ã£o

---

A memÃ³ria Ã© o maior gargalo oculto no software moderno de alta concorrÃªncia. Quanto mais prÃ³ximo o dado estÃ¡ da CPU, mais rÃ¡pido Ã© o acesso. Mas a velocidade custa dinheiro e escala tÃ©rmica.

---

---

<!-- .element: class="fragment" -->
# Novo TÃ³pico
## ğŸ›ï¸ 1. A PirÃ¢mide de Alta Performance

---

## ğŸ›ï¸ 1. A PirÃ¢mide de Alta Performance

Um programador ingÃªnuo acha que "variÃ¡vel vai na memÃ³ria". Um engenheiro de software C/C++ sabe *em qual camada* a variÃ¡vel se hospeda:

---

## ğŸ›ï¸ 1. A PirÃ¢mide de Alta Performance

```mermaid
graph TD
    A("Registradores<br/>(1 Ciclo - Alguns Bytes)") --> B["Cache L1<br/>(~4 Ciclos - ~64KB a 128KB)"]
    B --> C["Cache L2 e L3<br/>(~12 a ~40 Ciclos - Megabytes)"]
    C --> D[["RAM (MemÃ³ria Principal)<br/>(~200 a ~300 Ciclos - Gigabytes)"]]
    D --> E[("Armazenamento (SSD / HDD)<br/>(MilhÃµes de Ciclos - Terabytes)")]
    
    style A fill:#ff9999
    style B fill:#ffcc99
    style C fill:#ffff99
    style D fill:#ccffcc
    style E fill:#99ccff
```

---

## ğŸ›ï¸ 1. A PirÃ¢mide de Alta Performance

> [!IMPORTANT]
> A latÃªncia Ã© o tempo que demora da CPU pedir um dado atÃ© ele chegar. Buscar um byte da **RAM** demora ~200 ciclos. Buscar do **SSD** demora centenas de milhares. Essa diferenÃ§a grotesca Ã© mitigada pelo uso de Caches.

---

## ğŸ›ï¸ 1. A PirÃ¢mide de Alta Performance

---

---

<!-- .element: class="fragment" -->
# Novo TÃ³pico
## â³ 2. Os Impactos da LatÃªncia (Lado do CÃ³digo)

---

## â³ 2. Os Impactos da LatÃªncia (Lado do CÃ³digo)

Quando escrevemos um cÃ³digo com constantes consultas nÃ£o linearizadas ao Banco de Dados (ou SSD local), pagamos a mais cara taxa processual: o I/O disk penalty.

---

## â³ 2. Os Impactos da LatÃªncia (Lado do CÃ³digo)

<!-- termynal -->
```console
$ # Como consultar as camadas do processador Linux
$ lscpu | grep Cache
L1d cache:                       64 KiB
L1i cache:                       64 KiB
L2 cache:                        1 MiB
L3 cache:                        12 MiB
```

---

## â³ 2. Os Impactos da LatÃªncia (Lado do CÃ³digo)

A instruÃ§Ã£o e os dados descem da L3, saltam para L2, descem para L1 e se acoplam na ALU.

---

---

<!-- .element: class="fragment" -->
# Novo TÃ³pico
## ğŸ¯ 3. Optimizando Uso

---

## ğŸ¯ 3. Optimizando Uso

Por que linguagens como C e C++ dominam infraestrutura de servidores High Frequency Trading?
Porque elas permitem `AlocaÃ§Ã£o EstÃ¡tica e Constante` que Ã© perfeitamente "encaixada" pelo compilador diretamente na memÃ³ria **Cache**.

Ao invÃ©s de carregar gigabytes de *Strings* na lenta RAM, as linguagens de baixo nÃ­vel incentivam o uso de matrizes de tamanho delimitado (arrays fixos), cujo agrupamento contÃ­guo forÃ§a a arquitetura de **Hardware Prefetching** a adiantar os bytes do Array para a Cache nativamente, antes mesmo de vocÃª rodar a linha do cÃ³digo!

---

<!-- .element: class="fragment" -->
# Novo TÃ³pico
## ğŸš€ Resumo PrÃ¡tico

---

## ğŸš€ Resumo PrÃ¡tico

- Se processadores hoje sÃ£o mÃ­sseis atingindo +4GHz, a RAM parou no tempo (LatÃªncia de CAS nÃ£o baixa proporcionalmente).
- Tudo recai na tÃ©cnica humana de amarrar dados juntos (Caches L1 e L2) e escrever *data-oriented code* se quiser ultra-latÃªncia C++.

---

## ğŸš€ Resumo PrÃ¡tico

---